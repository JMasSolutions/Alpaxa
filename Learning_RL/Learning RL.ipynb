{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287f97a07a8179f4",
   "metadata": {},
   "source": [
    "# 1.Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import os\n",
    "from cmath import pi\n",
    "\n",
    "import gymnasium\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b32de1e03af6a0b9",
   "metadata": {},
   "source": [
    "# 2. Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec3f639f5d95cd91",
   "metadata": {},
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gymnasium.make(environment_name, render_mode=\"human\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d514f166f9c4f84",
   "metadata": {},
   "source": [
    "# episodes = 5\n",
    "# for episode in range(1, episodes + 1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score = 0\n",
    "# \n",
    "#     while not done:\n",
    "#         action = env.action_space.sample()\n",
    "#         n_state, reward, terminated, truncated, info = env.step(action)\n",
    "#         done = terminated or truncated\n",
    "#         score += reward\n",
    "# \n",
    "#     print(f'Episode:{episode}, Score:{score}')\n",
    "# env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b41950eaed66cb00",
   "metadata": {},
   "source": [
    "# 3.Understanding the Environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "5208f8ca5bee630b",
   "metadata": {},
   "source": [
    "env.action_space"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49b3825a5bc85dbf",
   "metadata": {},
   "source": [
    "env.action_space.sample()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "887f511dabe68de1",
   "metadata": {},
   "source": [
    "env.observation_space.sample()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc0acaa3c16162ab",
   "metadata": {},
   "source": [
    "# 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ff51ba4c8167df1",
   "metadata": {},
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8465188f29dab0c7",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS is not available, using CPU.\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7372fca2c8e99cb1",
   "metadata": {},
   "source": [
    "env = gymnasium.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e0066099b1dd138",
   "metadata": {},
   "source": [
    "model.learn(total_timesteps=20000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f5fe05b6eaf35118",
   "metadata": {},
   "source": [
    "# 5. Save Model & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "40778161c891ca5d",
   "metadata": {},
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model_CartPole')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3cd944054becb133",
   "metadata": {},
   "source": [
    "model.save(PPO_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f57aa76107431949",
   "metadata": {},
   "source": [
    "del model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d74dd9f4f1e8b8cd",
   "metadata": {},
   "source": [
    "model = PPO.load(PPO_path, env=env)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ded61735ea84195",
   "metadata": {},
   "source": [
    "# 6. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f3337fb9f45288d",
   "metadata": {},
   "source": [
    "# Step 1: Recreate the environment\n",
    "env = gymnasium.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Step 2: Define the path to the saved model\n",
    "PPO_path = os.path.join(\"Training\", \"Saved Models\", \"PPO_model_CartPole\")\n",
    "\n",
    "# Step 3: Load the saved model\n",
    "model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "# Step 4: Custom evaluation function\n",
    "def evaluate_policy_with_rendering(model, env, n_eval_episodes=10, render=True):\n",
    "    \"\"\"\n",
    "    Evaluate the policy of a loaded model with optional rendering.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        reset_output = env.reset()\n",
    "        state = reset_output[0] if isinstance(reset_output, tuple) else reset_output\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()  # Render each frame\n",
    "\n",
    "            # Get the action from the model\n",
    "            action, _states = model.predict(state, deterministic=True)\n",
    "            \n",
    "            # Adjust for environments returning 4 or 5 values\n",
    "            step_output = env.step(action)\n",
    "            if len(step_output) == 5:\n",
    "                state, reward, terminated, truncated, info = step_output\n",
    "            else:\n",
    "                state, reward, terminated, truncated = step_output\n",
    "\n",
    "            # Convert reward to scalar to avoid warnings\n",
    "            total_reward += reward.item() if hasattr(reward, 'item') else float(reward)\n",
    "\n",
    "            # Combine termination flags\n",
    "            done = terminated or truncated\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    # Calculate mean and standard deviation of rewards\n",
    "    mean_reward = sum(episode_rewards) / n_eval_episodes\n",
    "    std_reward = (sum([(x - mean_reward) ** 2 for x in episode_rewards]) / n_eval_episodes) ** 0.5\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# Step 5: Evaluate the loaded model\n",
    "mean_reward, std_reward = evaluate_policy_with_rendering(model, env, n_eval_episodes=10, render=True)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Test Model",
   "id": "6a10388191b973ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Define the path to the saved model\n",
    "PPO_path = os.path.join(\"Training\", \"Saved Models\", \"PPO_model_CartPole\")\n",
    "\n",
    "# Load the saved model\n",
    "model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "# Number of episodes\n",
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "    # Extract the observation from the reset output\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        score += reward\n",
    "\n",
    "    print(f'Episode: {episode}, Score: {score}')\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ],
   "id": "35c135133af76057",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8. View Logs in Tensorboard",
   "id": "9767a95fde3ce94d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "log_path = os.path.join('Training', 'Logs')\n",
    "training_log_path = os.path.join(log_path, 'PPO_2')\n"
   ],
   "id": "945a408411a8b8d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!tensorboard --logdir={training_log_path}",
   "id": "2b6436bd504b63a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 9. Adding a callback to the training stage",
   "id": "f5b04b412d658677"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Wrap the training environment with Monitor for proper episode length and reward tracking\n",
    "train_env = Monitor(env)\n",
    "\n",
    "# The path to save logs and models\n",
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "best_model_save_path = os.path.join(\"Training\", \"Saved_Models\")\n",
    "\n",
    "# Create the evaluation environment and wrap it with Monitor\n",
    "eval_env = gym.make(\"CartPole-v1\")\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "# Create the callback\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env,\n",
    "                             callback_on_new_best=stop_callback,\n",
    "                             eval_freq=10000,\n",
    "                             best_model_save_path=best_model_save_path,\n",
    "                             verbose=1)\n",
    "\n",
    "# Create the PPO model\n",
    "model = PPO('MlpPolicy', train_env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = os.path.join(\"Training\", \"Saved_Models\", \"ppo_cartpole\")\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(\"Training complete! Model saved at:\", model_save_path)\n",
    "\n",
    "# Test the trained model\n",
    "test_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "\n",
    "print(\"Testing the trained model...\")\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, _ = test_env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished!\")\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "test_env.close()"
   ],
   "id": "3b55c90fb25e3d1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 10. Change policy",
   "id": "aa7ab435c661f6fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T05:20:37.248346Z",
     "start_time": "2024-12-20T05:16:42.250271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Wrap the training environment with Monitor for proper episode length and reward tracking\n",
    "train_env = Monitor(env)\n",
    "\n",
    "# The path to save logs and models\n",
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "best_model_save_path = os.path.join(\"Training\", \"Saved_Models\")\n",
    "\n",
    "# Create the evaluation environment and wrap it with Monitor\n",
    "eval_env = gym.make(\"CartPole-v1\")\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "# Create the callback\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env,\n",
    "                             callback_on_new_best=stop_callback,\n",
    "                             eval_freq=10000,\n",
    "                             best_model_save_path=best_model_save_path,\n",
    "                             verbose=1)\n",
    "\n",
    "# Define the new policy architecture\n",
    "new_arch = dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])\n",
    "\n",
    "# Create the PPO model with the new policy architecture\n",
    "model = PPO('MlpPolicy', train_env, policy_kwargs={\"net_arch\": new_arch}, verbose=1, tensorboard_log=log_path, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = os.path.join(\"Training\", \"Saved_Models\", \"ppo_cartpole\")\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(\"Training complete! Model saved at:\", model_save_path)\n",
    "\n",
    "# Test the trained model\n",
    "test_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "\n",
    "print(\"Testing the trained model...\")\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, _ = test_env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished!\")\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "test_env.close()"
   ],
   "id": "5f2994c1d01245a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/9c4gw5hx1zdb2g73zj0vwl8w0000gn/T/ipykernel_6409/3545831138.py:9: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 13:16:46.772 Python[6409:263760] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-20 13:16:46.772 Python[6409:263760] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | 21.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28          |\n",
      "|    ep_rew_mean          | 28          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015097221 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | 0.00021     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.17        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 18.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 40.7        |\n",
      "|    ep_rew_mean          | 40.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020058561 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0384     |\n",
      "|    value_loss           | 23.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 57.5        |\n",
      "|    ep_rew_mean          | 57.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012443971 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.599      |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    value_loss           | 44.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=486.60 +/- 26.80\n",
      "Episode length: 486.60 +/- 26.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 487         |\n",
      "|    mean_reward          | 487         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019486286 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.35        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 31.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 486.60  is above the threshold 200\n",
      "Training complete! Model saved at: Training/Saved_Models/ppo_cartpole\n",
      "Testing the trained model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 55\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTesting the trained model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m---> 55\u001B[0m     action, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m     obs, reward, done, _ \u001B[38;5;241m=\u001B[39m test_env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:557\u001B[0m, in \u001B[0;36mBaseAlgorithm.predict\u001B[0;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    539\u001B[0m     observation: Union[np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    542\u001B[0m     deterministic: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    543\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[np\u001B[38;5;241m.\u001B[39mndarray, Optional[\u001B[38;5;28mtuple\u001B[39m[np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]]]:\n\u001B[1;32m    544\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    545\u001B[0m \u001B[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001B[39;00m\n\u001B[1;32m    546\u001B[0m \u001B[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;124;03m        (used in recurrent policies)\u001B[39;00m\n\u001B[1;32m    556\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 557\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisode_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stable_baselines3/common/policies.py:357\u001B[0m, in \u001B[0;36mBasePolicy.predict\u001B[0;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001B[39;00m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(observation, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(observation) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(observation[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m--> 357\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    358\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    359\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    360\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    361\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    362\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    363\u001B[0m     )\n\u001B[1;32m    365\u001B[0m obs_tensor, vectorized_env \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobs_to_tensor(observation)\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "\u001B[0;31mValueError\u001B[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 11. Utilising an Alternate Algorithim",
   "id": "c1b1295d4cadcdef"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-20T07:31:33.951150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, norm_reward=True)\n",
    "\n",
    "# Log and model paths\n",
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "model_save_path = os.path.join(\"Training\", \"Saved_Models\", \"dqn_cartpole\")\n",
    "\n",
    "# Create the DQN model\n",
    "model = DQN(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_path,\n",
    "    exploration_fraction=0.3,  # Slower exploration decay\n",
    "    exploration_final_eps=0.05,  # Ensure some exploration remains\n",
    "    buffer_size=100000,  # Larger replay buffer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(model_save_path)\n",
    "print(\"Training complete! Model saved at:\", model_save_path)\n",
    "\n",
    "# Test the trained model\n",
    "test_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, _ = test_env.reset()  # Correctly handle the tuple returned by reset()\n",
    "done = False\n",
    "\n",
    "print(\"Testing the trained model...\")\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)  # Only pass `obs` to predict()\n",
    "    obs, reward, done, truncated, info = test_env.step(action)  # Updated to handle 5 values\n",
    "    done = done or truncated  # Check if the episode is over (done or truncated)\n",
    "    if done:\n",
    "        print(\"Episode finished!\")\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "test_env.close()"
   ],
   "id": "addc87df6965c94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training/Logs/DQN_11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.997    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 8596     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 87       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.994    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 4442     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 178      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0154   |\n",
      "|    n_updates        | 19       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.991    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 4383     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 280      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00802  |\n",
      "|    n_updates        | 44       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.989    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 4407     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 341      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00437  |\n",
      "|    n_updates        | 60       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.987    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 4444     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 409      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00405  |\n",
      "|    n_updates        | 77       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.985    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 4438     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 474      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00179  |\n",
      "|    n_updates        | 93       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.982    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 4509     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 583      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00133  |\n",
      "|    n_updates        | 120      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.979    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 4503     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 666      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00313  |\n",
      "|    n_updates        | 141      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.976    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 4534     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 762      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00103  |\n",
      "|    n_updates        | 165      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 20.7     |\n",
      "|    exploration_rate | 0.974    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 4554     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 827      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00119  |\n",
      "|    n_updates        | 181      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.971    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 4577     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 914      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00193  |\n",
      "|    n_updates        | 203      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 20.7     |\n",
      "|    exploration_rate | 0.969    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 4536     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 992      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000734 |\n",
      "|    n_updates        | 222      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.967    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 4531     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1052     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00184  |\n",
      "|    n_updates        | 237      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 4529     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1124     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000587 |\n",
      "|    n_updates        | 255      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.961    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 4534     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1217     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00258  |\n",
      "|    n_updates        | 279      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.959    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 4538     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1280     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000544 |\n",
      "|    n_updates        | 294      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.957    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 4543     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1363     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000481 |\n",
      "|    n_updates        | 315      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.954    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 4560     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1453     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0022   |\n",
      "|    n_updates        | 338      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 20.5     |\n",
      "|    exploration_rate | 0.951    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 4573     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1561     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00109  |\n",
      "|    n_updates        | 365      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 20.7     |\n",
      "|    exploration_rate | 0.948    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 4588     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1657     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00171  |\n",
      "|    n_updates        | 389      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.944    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 4596     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1777     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00276  |\n",
      "|    n_updates        | 419      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.94     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 4595     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1896     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0013   |\n",
      "|    n_updates        | 448      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.937    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 4594     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1979     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00123  |\n",
      "|    n_updates        | 469      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.934    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 4597     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2081     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00157  |\n",
      "|    n_updates        | 495      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.932    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 4579     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2135     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000584 |\n",
      "|    n_updates        | 508      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.928    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 4594     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2262     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000263 |\n",
      "|    n_updates        | 540      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.925    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 4618     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2375     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00243  |\n",
      "|    n_updates        | 568      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.921    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 4628     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2482     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00124  |\n",
      "|    n_updates        | 595      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
