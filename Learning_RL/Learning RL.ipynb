{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287f97a07a8179f4",
   "metadata": {},
   "source": [
    "# 1.Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T03:58:50.521414Z",
     "start_time": "2024-12-20T03:58:47.126008Z"
    }
   },
   "source": [
    "import os\n",
    "import gymnasium\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "b32de1e03af6a0b9",
   "metadata": {},
   "source": [
    "# 2. Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec3f639f5d95cd91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T03:58:50.529785Z",
     "start_time": "2024-12-20T03:58:50.524893Z"
    }
   },
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gymnasium.make(environment_name, render_mode=\"human\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "8d514f166f9c4f84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T03:58:50.590992Z",
     "start_time": "2024-12-20T03:58:50.589577Z"
    }
   },
   "source": [
    "# episodes = 5\n",
    "# for episode in range(1, episodes + 1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score = 0\n",
    "# \n",
    "#     while not done:\n",
    "#         action = env.action_space.sample()\n",
    "#         n_state, reward, terminated, truncated, info = env.step(action)\n",
    "#         done = terminated or truncated\n",
    "#         score += reward\n",
    "# \n",
    "#     print(f'Episode:{episode}, Score:{score}')\n",
    "# env.close()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "b41950eaed66cb00",
   "metadata": {},
   "source": [
    "# 3.Understanding the Environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "5208f8ca5bee630b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T03:58:50.596993Z",
     "start_time": "2024-12-20T03:58:50.594522Z"
    }
   },
   "source": [
    "env.action_space"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "49b3825a5bc85dbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T03:58:50.658737Z",
     "start_time": "2024-12-20T03:58:50.656279Z"
    }
   },
   "source": [
    "env.action_space.sample()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "887f511dabe68de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T03:58:50.729261Z",
     "start_time": "2024-12-20T03:58:50.726790Z"
    }
   },
   "source": [
    "env.observation_space.sample()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.42379   , -1.4380158 , -0.23170045, -1.0911617 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "dc0acaa3c16162ab",
   "metadata": {},
   "source": [
    "# 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ff51ba4c8167df1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T03:58:50.771003Z",
     "start_time": "2024-12-20T03:58:50.769153Z"
    }
   },
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "8465188f29dab0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T03:58:50.788936Z",
     "start_time": "2024-12-20T03:58:50.775952Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS is not available, using CPU.\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "7372fca2c8e99cb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T03:58:51.351576Z",
     "start_time": "2024-12-20T03:58:50.792321Z"
    }
   },
   "source": [
    "env = gymnasium.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, device=device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "9e0066099b1dd138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T04:00:08.429291Z",
     "start_time": "2024-12-20T03:58:51.355493Z"
    }
   },
   "source": [
    "model.learn(total_timesteps=20000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 366  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 303          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069779074 |\n",
      "|    clip_fraction        | 0.0713       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.687       |\n",
      "|    explained_variance   | 0.000318     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0109      |\n",
      "|    value_loss           | 58.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 291         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009374195 |\n",
      "|    clip_fraction        | 0.0637      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.0562      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 42.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 285         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009901587 |\n",
      "|    clip_fraction        | 0.0936      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 52.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008873332 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 62          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010491421 |\n",
      "|    clip_fraction        | 0.0837      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 68.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 278          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067769224 |\n",
      "|    clip_fraction        | 0.0483       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.58        |\n",
      "|    explained_variance   | 0.709        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.19         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00746     |\n",
      "|    value_loss           | 38.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 276          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044623967 |\n",
      "|    clip_fraction        | 0.0453       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.572       |\n",
      "|    explained_variance   | 0.577        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.7         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00711     |\n",
      "|    value_loss           | 48           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 275         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008441398 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.51        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00577    |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 274          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058932975 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.576       |\n",
      "|    explained_variance   | 0.519        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.17         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00654     |\n",
      "|    value_loss           | 55.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x314fab7d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "f5fe05b6eaf35118",
   "metadata": {},
   "source": [
    "# 5. Save Model & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "40778161c891ca5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T04:00:08.490843Z",
     "start_time": "2024-12-20T04:00:08.489238Z"
    }
   },
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model_CartPole')"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "3cd944054becb133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T04:00:08.519900Z",
     "start_time": "2024-12-20T04:00:08.506731Z"
    }
   },
   "source": [
    "model.save(PPO_path)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "f57aa76107431949",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T04:00:08.526220Z",
     "start_time": "2024-12-20T04:00:08.524626Z"
    }
   },
   "source": [
    "del model"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "d74dd9f4f1e8b8cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T04:00:08.539078Z",
     "start_time": "2024-12-20T04:00:08.533460Z"
    }
   },
   "source": [
    "model = PPO.load(PPO_path, env=env)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "8ded61735ea84195",
   "metadata": {},
   "source": [
    "# 6. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f3337fb9f45288d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T04:03:40.787684Z",
     "start_time": "2024-12-20T04:00:08.544160Z"
    }
   },
   "source": [
    "# Step 1: Recreate the environment\n",
    "env = gymnasium.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Step 2: Define the path to the saved model\n",
    "PPO_path = os.path.join(\"Training\", \"Saved Models\", \"PPO_model_CartPole\")\n",
    "\n",
    "# Step 3: Load the saved model\n",
    "model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "# Step 4: Custom evaluation function\n",
    "def evaluate_policy_with_rendering(model, env, n_eval_episodes=10, render=True):\n",
    "    \"\"\"\n",
    "    Evaluate the policy of a loaded model with optional rendering.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        reset_output = env.reset()\n",
    "        state = reset_output[0] if isinstance(reset_output, tuple) else reset_output\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()  # Render each frame\n",
    "\n",
    "            # Get the action from the model\n",
    "            action, _states = model.predict(state, deterministic=True)\n",
    "            \n",
    "            # Adjust for environments returning 4 or 5 values\n",
    "            step_output = env.step(action)\n",
    "            if len(step_output) == 5:\n",
    "                state, reward, terminated, truncated, info = step_output\n",
    "            else:\n",
    "                state, reward, terminated, truncated = step_output\n",
    "\n",
    "            # Convert reward to scalar to avoid warnings\n",
    "            total_reward += reward.item() if hasattr(reward, 'item') else float(reward)\n",
    "\n",
    "            # Combine termination flags\n",
    "            done = terminated or truncated\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    # Calculate mean and standard deviation of rewards\n",
    "    mean_reward = sum(episode_rewards) / n_eval_episodes\n",
    "    std_reward = (sum([(x - mean_reward) ** 2 for x in episode_rewards]) / n_eval_episodes) ** 0.5\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# Step 5: Evaluate the loaded model\n",
    "mean_reward, std_reward = evaluate_policy_with_rendering(model, env, n_eval_episodes=10, render=True)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 12:00:09.214 Python[5432:179914] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-20 12:00:09.215 Python[5432:179914] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 500.0\n",
      "Episode 2: Total Reward = 500.0\n",
      "Episode 3: Total Reward = 500.0\n",
      "Episode 4: Total Reward = 500.0\n",
      "Episode 5: Total Reward = 500.0\n",
      "Episode 6: Total Reward = 500.0\n",
      "Episode 7: Total Reward = 500.0\n",
      "Episode 8: Total Reward = 500.0\n",
      "Episode 9: Total Reward = 500.0\n",
      "Episode 10: Total Reward = 500.0\n",
      "Mean reward: 500.0, Std reward: 0.0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T04:03:40.806069Z",
     "start_time": "2024-12-20T04:03:40.804152Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "35c135133af76057",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
